<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>博客</title>
    <link href="https://jie2gg.github.io/feed.xml" rel="self" />
    <link href="https://jie2gg.github.io" />
    <updated>2024-08-16T16:28:16+08:00</updated>
    <author>
        <name>JieGG</name>
    </author>
    <id>https://jie2gg.github.io</id>

    <entry>
        <title>大模型部署(三)——模型启动</title>
        <author>
            <name>JieGG</name>
        </author>
        <link href="https://jie2gg.github.io/da-mo-xing-bu-shusan-mo-xing-qi-dong.html"/>
        <id>https://jie2gg.github.io/da-mo-xing-bu-shusan-mo-xing-qi-dong.html</id>
            <category term="AI"/>

        <updated>2024-08-16T15:49:32+08:00</updated>
            <summary>
                <![CDATA[
                    本文介绍 由于大模型的复杂性和计算需求，启动和运行这些模型往往需要大量的计算资源和高效的分布式架构。Xinference 和 vLLM 是两个旨在简化大模型部署和提高推理性能的工具。 1. 模型下载 首先将模型所需的模型下载至本地，通常开源的 LLM 模型可以从 HuggingFace 或 ModelScope 下载。 以本文章中使用的 LLM 模型 Qwen2-72B-Instruct 为例： 下载模型需要先安装 Git LFS # 安装 git-lfs $ apt-get install git-lfs -y # 下载所需的模型 $ git clone https://www.modelscope.cn/models/qwen/qwen2-72b-instruct 2. 模型启动 借助 vLLM，构建一个与 OpenAI API 兼容的 API 服务十分简便，该服务可以作为实现OpenAI&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h3 id="本文介绍">本文介绍</h3>
<p>由于大模型的复杂性和计算需求，启动和运行这些模型往往需要大量的计算资源和高效的分布式架构。Xinference 和 vLLM 是两个旨在简化大模型部署和提高推理性能的工具。</p><h3 id="vllm集群部署">vLLM集群部署</h3>
<p><strong>1. 模型下载</strong></p><p>首先将模型所需的模型下载至本地，通常开源的 LLM 模型可以从 <a href="https://huggingface.co/models">HuggingFace</a> 或 <a href="https://www.modelscope.cn/">ModelScope</a> 下载。</p><p>以本文章中使用的 LLM 模型 <a href="https://www.modelscope.cn/models/qwen/qwen2-72b-instruct">Qwen2-72B-Instruct</a> 为例：</p><p>下载模型需要先安装 <a href="https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage">Git LFS</a></p><pre><code class="language-bash"># 安装 git-lfs
$ apt-get install git-lfs -y

# 下载所需的模型
$ git clone https://www.modelscope.cn/models/qwen/qwen2-72b-instruct
</code></pre>
<p><strong>2. 模型启动</strong></p><p>借助 vLLM，构建一个与 OpenAI API 兼容的 API 服务十分简便，该服务可以作为实现OpenAI API协议的服务器进行部署。并且 vLLM 原生支持 Ray 集群，使用以下命令可以让大模型以集群的方式启动：</p><pre><code class="language-bash"># 多卡分布式启动 OpenAI API 服务器
$ python -m vllm.entrypoints.openai.api_server \
      --served-model-name Qwen2-72B-Instruct \
            --model path/to/weight \
            --pipeline-parallel-size 2 \
            --tensor-parallel-size 1 \
            --gpu-memory-utilization 0.98 \
            --max-model-len 32768 \
</code></pre>
<p>参数说明：</p><ul>
<li>served-model-name 模型ID，请求时的模型名称</li>
<li>model 模型路径，模型在集群上的位置</li>
<li>pipeline-parallel-size 管线数量，例：2个节点设置为2</li>
<li>tensor-parallel-size 节点GPU卡数量，每个节点的GPU数量</li>
<li>gpu-memory-utilization GPU显存占用量，最大值 1。0.98 表示占用98%</li>
<li>max-model-len 最大模型大小，限制模型的尺寸以降低显存占用</li>
</ul>
<h3 id="xinference集群部署">Xinference集群部署</h3>
<p><strong>1. 访问平台</strong>
访问 Supervisor 节点，在浏览器中访问 “<a href="http://SUPERVISOR_HOST:9997">http://SUPERVISOR_HOST:9997</a>“ （SUPERVISOR_HOST 替换为 Supervisor 的IP地址）
界面如下：
<figure class="post__image"><img loading="lazy" src="https://jie2gg.github.io/media/posts/6/xinference-ui-2.png" alt="xinference-ui" width="2914" height="1726"  sizes="(max-width: 48em) 100vw, 768px" srcset="https://jie2gg.github.io/media/posts/6/responsive/xinference-ui-2-xs.png 300w ,https://jie2gg.github.io/media/posts/6/responsive/xinference-ui-2-sm.png 480w ,https://jie2gg.github.io/media/posts/6/responsive/xinference-ui-2-md.png 768w ,https://jie2gg.github.io/media/posts/6/responsive/xinference-ui-2-lg.png 1024w ,https://jie2gg.github.io/media/posts/6/responsive/xinference-ui-2-xl.png 1360w ,https://jie2gg.github.io/media/posts/6/responsive/xinference-ui-2-2xl.png 1600w"></figure></p><p><strong>2. 启动模型</strong>
点击要启动的模型，选择 vLLM 引擎，设置模型参数（可参考 vLLM 的启动参数），点击启动图标后 xinference 会自动下载并部署模型。
<figure class="post__image"><img loading="lazy" src="https://jie2gg.github.io/media/posts/6/xinference-launch.png" alt="xinference-launch" width="2914" height="1726"  sizes="(max-width: 48em) 100vw, 768px" srcset="https://jie2gg.github.io/media/posts/6/responsive/xinference-launch-xs.png 300w ,https://jie2gg.github.io/media/posts/6/responsive/xinference-launch-sm.png 480w ,https://jie2gg.github.io/media/posts/6/responsive/xinference-launch-md.png 768w ,https://jie2gg.github.io/media/posts/6/responsive/xinference-launch-lg.png 1024w ,https://jie2gg.github.io/media/posts/6/responsive/xinference-launch-xl.png 1360w ,https://jie2gg.github.io/media/posts/6/responsive/xinference-launch-2xl.png 1600w"></figure></p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>大模型部署(二)——集群推理框架安装</title>
        <author>
            <name>JieGG</name>
        </author>
        <link href="https://jie2gg.github.io/da-mo-xing-bu-shuer-tui-li-kuang-jia-an-zhuang.html"/>
        <id>https://jie2gg.github.io/da-mo-xing-bu-shuer-tui-li-kuang-jia-an-zhuang.html</id>
            <category term="AI"/>

        <updated>2024-08-16T11:33:40+08:00</updated>
            <summary>
                <![CDATA[
                    本文介绍 在部署大规模预训练模型时，选择合适的集群框架至关重要。本文将重点介绍 Xinference 和 Ray 这两个高效且易用的集群框架，并提供简要的安装指南。 在选择 Ray 集群和 Xinference 集群时，重要的是要考虑您的具体需求和应用场景。 Ray 是一个通用的分布式计算框架，支持多种类型的工作负载，包括机器学习、深度学习、强化学习等。 Xinference 是一个专为大模型推理优化的分布式推理框架。它特别设计用于提供高性能的推理服务，支持模型并行和数据并行，适用于需要高性能和低延迟响应的应用场景。 在选择集群时，如果您的项目需要一个通用的分布式计算框架来处理多种类型的任务，那么 Ray 将是一个理想的选择。而如果您专注于模型推理，并且需要一个高度优化的推理服务，那么 Xinference 将是更好的选择。根据您的具体需求和应用场景来决定使用哪一个集群，可以确保您的项目获得最佳性能和效率。 Ray 只是一个通用的框架，本身不能运行大语言模型。如果需要运行大模型，还需要使用 vLLM 借助 Ray 的能力进行分布式推理。 1. 环境准备： 2. 安装依赖： # 安装 python3 开发套件，虚拟环境 $ apt-get install python3-dev python3-venv libopenblas-dev 3. 安装Ray框架 # 创建 python3&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h3 id="本文介绍">本文介绍</h3>
<p>在部署大规模预训练模型时，选择合适的集群框架至关重要。本文将重点介绍 Xinference 和 Ray 这两个高效且易用的集群框架，并提供简要的安装指南。</p><h3 id="框架简介">框架简介</h3>
<p>在选择 Ray 集群和 Xinference 集群时，重要的是要考虑您的具体需求和应用场景。</p><ul>
<li><p>Ray 是一个通用的分布式计算框架，支持<strong>多种类型的工作负载</strong>，包括<strong>机器学习、深度学习、强化学习</strong>等。</p></li>
<li><p>Xinference 是一个专为大模型推理优化的分布式推理框架。它特别设计用于提供<strong>高性能的推理服务</strong>，支持<strong>模型并行和数据并行</strong>，适用于需要高性能和低延迟响应的应用场景。</p></li>
</ul>
<h3 id="框架选择">框架选择</h3>
<p>在选择集群时，如果您的项目需要一个通用的分布式计算框架来处理多种类型的任务，那么 Ray 将是一个理想的选择。而如果您专注于模型推理，并且需要一个高度优化的推理服务，那么 Xinference 将是更好的选择。根据您的具体需求和应用场景来决定使用哪一个集群，可以确保您的项目获得最佳性能和效率。</p><h3 id="ray集群部署">Ray集群部署</h3>
<p>Ray 只是一个通用的框架，本身不能运行大语言模型。如果需要运行大模型，还需要使用 vLLM 借助 Ray 的能力进行分布式推理。</p><p><strong>1. 环境准备：</strong></p><ul>
<li>确保您有一台或多台服务器或虚拟机，这些机器将被用作 Ray 集群的节点。</li>
<li>所有的机器都应该安装了相同版本的 Python（Ubuntu23.10自带Python3.11）</li>
</ul>
<p><strong>2. 安装依赖：</strong></p><pre><code class="language-bash"># 安装 python3 开发套件，虚拟环境
$ apt-get install python3-dev python3-venv libopenblas-dev
</code></pre>
<p><strong>3. 安装Ray框架</strong></p><pre><code class="language-bash"># 创建 python3 虚拟环境
$ python3 -m venv ray

# 激活虚拟环境
$ source ray/bin/activate

# 更新 pip3 版本并安装 ray 框架
$ pip3 install --upgrade pip
$ pip3 install &quot;ray[default]&quot;
</code></pre>
<p><strong>4. 安装vllm框架</strong></p><pre><code class="language-bash">$ pip3 install vllm
</code></pre>
<p><strong>5. 启动集群</strong></p><p>Ray 分布式集群由几个关键节点构成，其中最重要的是 Head 节点和 Worker 节点。</p><ul>
<li><strong>Head 节点</strong></li>
</ul>
<p>Head 节点是 Ray 集群的控制中心，负责以下关键功能：</p><ol>
<li>集群管理：维护集群的状态，包括跟踪哪些 worker 节点已经连接、资源分配情况等。</li>
<li>任务调度：接收任务请求，根据各个 worker 节点的资源状况智能调度任务到合适的节点上执行。</li>
<li>对象存储：管理一个分布式对象存储系统，使得在不同节点上运行的任务可以高效地共享数据。</li>
<li>服务发现：提供一个接入点让新加入的 worker 节点或其他服务能够发现集群并加入。</li>
</ol>
<p>选中一台服务器作为 Head 节点，使用以下命令启动 Ray 集群服务</p><pre><code class="language-bash"># 启动 Ray 集群的 Head 节点
$ ray start --head --port=6379 --dashboard-host=0.0.0.0 --dashboard-port=8265
</code></pre>
<ul>
<li><strong>Worker 节点</strong></li>
</ul>
<p>Worker 节点是执行实际计算任务的单元，它们连接到 Head 节点并根据 Head 节点的指令执行以下操作：</p><ol>
<li>任务执行：接收 Head 节点分配的任务，执行计算任务并返回结果。</li>
<li>资源报告：向 Head 节点报告自身可用的资源（如：CPU、GPU、内存等），以便进行任务调度。</li>
<li>对象存储：从对象存储中获取任务所需的数据，并将执行结果写回，支持跨节点的数据共享。</li>
</ol>
<p>其余的服务器作为 Worker 节点，使用以下命令启动 Ray 集群服务</p><pre><code class="language-bash"># 启动 Ray 集群的 Worker 节点
# HEAD_IP 是实际的 Head 节点的 IP 地址
$ ray start --address=&#39;HEAD_IP:6379&#39;
</code></pre>
<h3 id="xinference集群部署">Xinference集群部署</h3>
<p>Xinference 是专用于大模型的框架，内置了包含 vLLM 在内的多种推理引擎，支持部署包括 LLM，VLM，Embdding，Reranker 多种模型。</p><p><strong>1. 环境准备：</strong></p><ul>
<li>确保您有一台或多台服务器或虚拟机，这些机器将被用作 Xinference 集群的节点。</li>
<li>所有的机器都应该安装了相同版本的 Python（Ubuntu23.10自带Python3.11）</li>
</ul>
<p><strong>2. 安装依赖：</strong></p><pre><code class="language-bash"># 安装 python3 开发套件，虚拟环境
$ apt-get install python3-dev python3-venv libopenblas-dev
</code></pre>
<p><strong>3. 安装Xinference框架</strong></p><pre><code class="language-bash"># 创建 python3 虚拟环境
$ python3 -m venv xinference

# 激活虚拟环境
$ source xinference/bin/activate

# 更新 pip3 版本并安装 xinference 框架
$ pip3 install --upgrade pip
$ pip3 install &quot;xinference[all]&quot;
</code></pre>
<p><strong>4. 启动集群</strong></p><p>Xinference 集群由几个关键节点构成，其中最重要的是 Supervisor 节点和 Worker 节点。</p><ul>
<li><strong>Supervisor 节点</strong></li>
</ul>
<p>Supervisor 节点是 Xinference 集群的控制中心，负责以下关键功能：</p><ol>
<li>任务调度：Supervisor 节点负责调度推理任务到集群中的工作节点上执行。</li>
<li>资源管理：它管理集群中可用的计算资源（如 CPU、GPU、内存等），并根据任务需求动态分配资源。</li>
<li>状态监控：Supervisor 节点监控集群中所有节点的状态，确保任务正常运行，并在出现问题时采取相应的行动。</li>
<li>故障恢复：当工作节点出现故障时，Supervisor 节点能够检测到这一情况，并重新调度任务到其他可用节点上。</li>
<li>配置管理：Supervisor 节点还可能负责配置管理，包括设置每个节点的运行参数和环境变量。</li>
<li>性能优化：根据集群的负载情况，Supervisor 节点可以进行动态调整以优化整体性能。</li>
<li>API 接口：提供 API 接口供外部系统查询集群状态、提交任务、获取结果等。</li>
</ol>
<p><code>⚠️注意：Supervisor 节点只负责调度及提供API接口</code><br/>
选中一台服务器作为 Supervisor 节点，使用以下命令启动 Xinference 集群服务</p><pre><code class="language-bash"># 启动 Xinference 集群的 Supervisor 节点，SUPERVISOR_HOST 替换为当前节点内网IP
$ xinference-supervisor -H &quot;SUPERVISOR_HOST&quot;
</code></pre>
<ul>
<li><strong>Worker 节点</strong></li>
</ul>
<p>Worker 节点是执行实际计算任务的单元，它们连接到 Supervisor 节点并根据 Supervisor 节点的指令执行以下操作：</p><ol>
<li>模型部署：接收 Supervisor 节点分配的任务，从网络下载模型并部署。</li>
<li>资源报告：向 Supervisor 节点报告自身可用的资源（如：CPU、GPU、内存等），以便进行任务调度。</li>
</ol>
<p>其余的服务器作为 Worker 节点，使用以下命令启动 Xinference 集群服务</p><pre><code class="language-bash"># 启动 Xinference 集群的 Worker 节点，SUPERVISOR_HOST替换为 Supervisor 节点内网IP，WORKER_HOST 替换为当前接待内网IP
$ xinference-worker -e &quot;http://SUPERVISOR_HOST:9997&quot; -H &quot;WORKER_HOST&quot;
</code></pre>

            ]]>
        </content>
    </entry>
    <entry>
        <title>大模型部署(一)——服务器环境安装</title>
        <author>
            <name>JieGG</name>
        </author>
        <link href="https://jie2gg.github.io/ji-qun-bu-shu-da-mo-xingyi-fu-wu-qi-huan-jing-an-zhuang.html"/>
        <id>https://jie2gg.github.io/ji-qun-bu-shu-da-mo-xingyi-fu-wu-qi-huan-jing-an-zhuang.html</id>
            <category term="AI"/>

        <updated>2024-08-16T09:44:09+08:00</updated>
            <summary>
                <![CDATA[
                    本文介绍 在部署基于 NVIDIA GPU 的大语言模型服务器时，确保系统具备合适的驱动程序和软件环境至关重要。首先，安装 NVIDIA GPU 驱动程序以启用 GPU 加速功能；接着，安装 CUDA 工具包来提供必要的编程接口和支持库；最后，安装 cuDNN 以加速深度神经网络的训练过程。这些步骤共同构成了一个高效的深度学习开发环境，能够显著提升模型训练和推理的速度。 ⚠️注意：本文默认你会Linux操作基础，如果不会请先行学习Linux操作系统 ⚠️注意：本文默认你会Linux操作基础，如果不会请先行学习Linux操作系统 ⚠️注意：本文默认你会Linux操作基础，如果不会请先行学习Linux操作系统 本文章使用的软件版本将以本文编写时的最新版本作为演示（使用其他版本请注意安装方式），以下是所需的驱动软件的版本号。 1. 备份原文件 $ cd /etc/apt $ mv sources.list sources.list.bak 2. 创建新文件 $ touch sources.list 3. 在文件内写入下述源地址 # 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释 deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic main restricted universe&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                <h3 id="本文介绍">本文介绍</h3>
<p>在部署基于 NVIDIA GPU 的大语言模型服务器时，确保系统具备合适的驱动程序和软件环境至关重要。首先，安装 NVIDIA GPU 驱动程序以启用 GPU 加速功能；接着，安装 CUDA 工具包来提供必要的编程接口和支持库；最后，安装 cuDNN 以加速深度神经网络的训练过程。这些步骤共同构成了一个高效的深度学习开发环境，能够显著提升模型训练和推理的速度。</p><p><code>⚠️注意：本文默认你会Linux操作基础，如果不会请先行学习Linux操作系统</code><br/>
<code>⚠️注意：本文默认你会Linux操作基础，如果不会请先行学习Linux操作系统</code><br/>
<code>⚠️注意：本文默认你会Linux操作基础，如果不会请先行学习Linux操作系统</code><br/></p><h3 id="软件版本">软件版本</h3>
<p>本文章使用的软件版本将以本文编写时的最新版本作为演示（<strong>使用其他版本请注意安装方式</strong>），以下是所需的驱动软件的版本号。</p><ul>
<li>操作系统：<a href="https://cloud-images.ubuntu.com/releases/23.10/release-20240710/">Ubuntu 23.10</a></li>
<li>GPU驱动：<a href="https://cn.download.nvidia.com/tesla/550.90.07/NVIDIA-Linux-x86_64-550.90.07.run">550.90.07</a></li>
<li>CUDA-Toolkit：<a href="https://developer.download.nvidia.com/compute/cuda/12.6.0/local_installers/cuda_12.6.0_560.28.03_linux.run">12.6.0_560.28.03</a></li>
<li>cuDNN：<a href="https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb">9.3.0</a></li>
</ul>
<h3 id="换源">换源</h3>
<p><strong>1. 备份原文件</strong></p><pre><code class="language-bash">$ cd /etc/apt
$ mv sources.list sources.list.bak
</code></pre>
<p><strong>2. 创建新文件</strong></p><pre><code class="language-bash">$ touch sources.list
</code></pre>
<p><strong>3. 在文件内写入下述源地址</strong></p><pre><code class="language-text"># 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic main restricted universe multiverse
deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic-updates main restricted universe multiverse
deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic-updates main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic-backports main restricted universe multiverse
deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic-backports main restricted universe multiverse

# 以下安全更新软件源包含了官方源与镜像站配置，如有需要可自行修改注释切换
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic-security main restricted universe multiverse
deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic-security main restricted universe multiverse

# 预发布软件源，不建议启用
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic-proposed main restricted universe multiverse
deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ mantic-proposed main restricted universe multiverse
</code></pre>
<p><strong>4. 更新源和操作系统</strong></p><pre><code class="language-bash">$ apt-get update
$ apt-get upgrade -y
</code></pre>
<h3 id="安装基本依赖">安装基本依赖</h3>
<pre><code class="language-bash">$ apt-get update
$ $ apt-get install -y build-essential dkms mdevctl cmake make wget curl
</code></pre>
<h3 id="安装驱动程序、cuda-toolkit、cudnn">安装驱动程序、CUDA-Toolkit、cuDNN</h3>
<p><strong>1. 下载软件包（如果你选择上传则忽略此步骤）</strong></p><pre><code class="language-bash"># 下载驱动
$ wget https://cn.download.nvidia.com/tesla/550.90.07/NVIDIA-Linux-x86_64-550.90.07.run

# 下载cuda-toolkit
$ wget https://developer.download.nvidia.com/compute/cuda/12.6.0/local_installers/cuda_12.6.0_560.28.03_linux.run

# 下载cuDNN
$ wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb

# 设置权限
$ chmod +wx NVIDIA-Linux-x86_64-550.90.07.run
$ chmod +wx cuda_12.6.0_560.28.03_linux.run
</code></pre>
<p><strong>2. 安装驱动程序</strong></p><pre><code class="language-bash"># 安装 NVIDIA GPU驱动
$ ./NVIDIA-Linux-x86_64-550.90.07.run
</code></pre>
<ul>
<li><p>安装过程
<figure class="post__image"><img loading="lazy" src="https://jie2gg.github.io/media/posts/4/install-gpu-driver-01.png" alt="install-gpu-driver-01" width="2348" height="1320"  sizes="(max-width: 48em) 100vw, 768px" srcset="https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-01-xs.png 300w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-01-sm.png 480w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-01-md.png 768w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-01-lg.png 1024w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-01-xl.png 1360w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-01-2xl.png 1600w"></figure></p></li>
<li><p>使用遇到DKMS相关的选择YES
<figure class="post__image"><img loading="lazy" src="https://jie2gg.github.io/media/posts/4/install-gpu-driver-03-2.png" alt="install-gpu-driver-02" width="2338" height="1330"  sizes="(max-width: 48em) 100vw, 768px" srcset="https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-03-2-xs.png 300w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-03-2-sm.png 480w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-03-2-md.png 768w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-03-2-lg.png 1024w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-03-2-xl.png 1360w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-03-2-2xl.png 1600w"></figure></p></li>
<li><p>安装成功
<figure class="post__image"><img loading="lazy" src="https://jie2gg.github.io/media/posts/4/install-gpu-driver-04.png" alt="install-gpu-driver-03" width="2362" height="1336"  sizes="(max-width: 48em) 100vw, 768px" srcset="https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-04-xs.png 300w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-04-sm.png 480w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-04-md.png 768w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-04-lg.png 1024w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-04-xl.png 1360w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-04-2xl.png 1600w"></figure></p></li>
</ul>
<p><strong>3. 检查GPU状态</strong></p><pre><code class="language-bash"># 使用 nvidia-smi 检查 gpu 状态
$ nvidia-smi
</code></pre>
<p>正常情况下应该输出类似结果
<figure class="post__image"><img loading="lazy" src="https://jie2gg.github.io/media/posts/4/install-gpu-driver-05.png" alt="install-gpu-driver-04" width="1390" height="990"  sizes="(max-width: 48em) 100vw, 768px" srcset="https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-05-xs.png 300w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-05-sm.png 480w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-05-md.png 768w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-05-lg.png 1024w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-05-xl.png 1360w ,https://jie2gg.github.io/media/posts/4/responsive/install-gpu-driver-05-2xl.png 1600w"></figure></p><p><strong>4. 安装 CUDA 工具包</strong></p><pre><code class="language-bash"># 安装 cuda-toolkit
$ ./cuda_12.6.0_560.28.03_linux.run
</code></pre>
<ul>
<li><p>输入 accept 后开始安装
<figure class="post__image"><img loading="lazy" src="https://jie2gg.github.io/media/posts/4/install-cuda-01.png" alt="install-cuda-01" width="1210" height="990"  sizes="(max-width: 48em) 100vw, 768px" srcset="https://jie2gg.github.io/media/posts/4/responsive/install-cuda-01-xs.png 300w ,https://jie2gg.github.io/media/posts/4/responsive/install-cuda-01-sm.png 480w ,https://jie2gg.github.io/media/posts/4/responsive/install-cuda-01-md.png 768w ,https://jie2gg.github.io/media/posts/4/responsive/install-cuda-01-lg.png 1024w ,https://jie2gg.github.io/media/posts/4/responsive/install-cuda-01-xl.png 1360w ,https://jie2gg.github.io/media/posts/4/responsive/install-cuda-01-2xl.png 1600w"></figure></p></li>
<li><p>取消选择 driver，开始安装
<figure class="post__image"><img loading="lazy" src="https://jie2gg.github.io/media/posts/4/install-cuda-02.png" alt="install-cuda-02" width="1204" height="980"  sizes="(max-width: 48em) 100vw, 768px" srcset="https://jie2gg.github.io/media/posts/4/responsive/install-cuda-02-xs.png 300w ,https://jie2gg.github.io/media/posts/4/responsive/install-cuda-02-sm.png 480w ,https://jie2gg.github.io/media/posts/4/responsive/install-cuda-02-md.png 768w ,https://jie2gg.github.io/media/posts/4/responsive/install-cuda-02-lg.png 1024w ,https://jie2gg.github.io/media/posts/4/responsive/install-cuda-02-xl.png 1360w ,https://jie2gg.github.io/media/posts/4/responsive/install-cuda-02-2xl.png 1600w"></figure></p></li>
<li><p>安装成功
<figure class="post__image"><img loading="lazy" src="https://jie2gg.github.io/media/posts/4/install-cuda-03.png" alt="install-cuda-03" width="2382" height="716"  sizes="(max-width: 48em) 100vw, 768px" srcset="https://jie2gg.github.io/media/posts/4/responsive/install-cuda-03-xs.png 300w ,https://jie2gg.github.io/media/posts/4/responsive/install-cuda-03-sm.png 480w ,https://jie2gg.github.io/media/posts/4/responsive/install-cuda-03-md.png 768w ,https://jie2gg.github.io/media/posts/4/responsive/install-cuda-03-lg.png 1024w ,https://jie2gg.github.io/media/posts/4/responsive/install-cuda-03-xl.png 1360w ,https://jie2gg.github.io/media/posts/4/responsive/install-cuda-03-2xl.png 1600w"></figure></p></li>
<li><p>设置环境变量</p></li>
</ul>
<pre><code class="language-bash"># 这里的版本根据安装成功后提示的路径进行设置
$ echo &#39;export PATH=$PATH:/usr/local/cuda-12.6/bin&#39; &gt;&gt; ~/.bashrc
$ echo &#39;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-12.6/lib64&#39; &gt;&gt; ~/.bashrc
$ source ~/.bashrc
</code></pre>
<p><strong>5. 检查 CUDA 状态</strong></p><pre><code class="language-bash"># 使用 nvcc 检查版本
$ nvcc -V
</code></pre>
<p>正常情况应该输出如下结果</p><pre><code class="language-log">nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Fri_Jun_14_16:34:21_PDT_2024
Cuda compilation tools, release 12.6, V12.6.20
Build cuda_12.6.r12.6/compiler.34431801_0
</code></pre>
<p><strong>6. 安装 cuDNN 神经网络库</strong></p><pre><code class="language-bash"># 使用 dpkg 安装 cudnn 源
$ dpkg -i cuda-keyring_1.1-1_all.deb

# 更新软件包并安装 cudnn
$ apt-get update
$ apt-get install -y cudnn-cuda-12
</code></pre>

            ]]>
        </content>
    </entry>
</feed>
