<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Qwen1.5大模型分布式部署 - 博客</title><meta name="description" content="概述 在当今人工智能的迅猛发展浪潮中，我们正见证着前所未有的模型规模与复杂度的飞跃。伴随着模型尺寸的指数级增长，传统的单机部署模式已渐渐显得捉襟见肘，面临着计算资源瓶颈、训练时间过长以及推理效率低下等诸多挑战。 分布式部署作为解锁大模型潜力的金钥匙。通过将模型分布在多个计算节点上并行处理，在推理阶段提供更高的吞吐量和更低的延迟，确保服务的可扩展性和可靠性。此外，分布式系统还能通过灵活的资源调配机制，实现成本效益的最大化。 本系列博客，将利用强大的 vLLM 推理库与灵活的 Ray 集群，演示如何分布式部署 Qwen 大模型。 要顺利部署集群, 请按照以下系统要求进行配置&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://jie2gg.github.io/qwen15-14b-chatfen-bu-shi-bu-shu.html"><link rel="alternate" type="application/atom+xml" href="https://jie2gg.github.io/feed.xml"><link rel="alternate" type="application/json" href="https://jie2gg.github.io/feed.json"><meta property="og:title" content="Qwen1.5大模型分布式部署"><meta property="og:site_name" content="博客"><meta property="og:description" content="概述 在当今人工智能的迅猛发展浪潮中，我们正见证着前所未有的模型规模与复杂度的飞跃。伴随着模型尺寸的指数级增长，传统的单机部署模式已渐渐显得捉襟见肘，面临着计算资源瓶颈、训练时间过长以及推理效率低下等诸多挑战。 分布式部署作为解锁大模型潜力的金钥匙。通过将模型分布在多个计算节点上并行处理，在推理阶段提供更高的吞吐量和更低的延迟，确保服务的可扩展性和可靠性。此外，分布式系统还能通过灵活的资源调配机制，实现成本效益的最大化。 本系列博客，将利用强大的 vLLM 推理库与灵活的 Ray 集群，演示如何分布式部署 Qwen 大模型。 要顺利部署集群, 请按照以下系统要求进行配置&hellip;"><meta property="og:url" content="https://jie2gg.github.io/qwen15-14b-chatfen-bu-shi-bu-shu.html"><meta property="og:type" content="article"><link rel="stylesheet" href="https://jie2gg.github.io/assets/css/style.css?v=13cb51b9862d75783d9c7d9d0ae030a5"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://jie2gg.github.io/qwen15-14b-chatfen-bu-shi-bu-shu.html"},"headline":"Qwen1.5大模型分布式部署","datePublished":"2024-04-25T23:49","dateModified":"2024-04-26T10:36","description":"概述 在当今人工智能的迅猛发展浪潮中，我们正见证着前所未有的模型规模与复杂度的飞跃。伴随着模型尺寸的指数级增长，传统的单机部署模式已渐渐显得捉襟见肘，面临着计算资源瓶颈、训练时间过长以及推理效率低下等诸多挑战。 分布式部署作为解锁大模型潜力的金钥匙。通过将模型分布在多个计算节点上并行处理，在推理阶段提供更高的吞吐量和更低的延迟，确保服务的可扩展性和可靠性。此外，分布式系统还能通过灵活的资源调配机制，实现成本效益的最大化。 本系列博客，将利用强大的 vLLM 推理库与灵活的 Ray 集群，演示如何分布式部署 Qwen 大模型。 要顺利部署集群, 请按照以下系统要求进行配置&hellip;","author":{"@type":"Person","name":"JieGG","url":"https://jie2gg.github.io/authors/jiegg/"},"publisher":{"@type":"Organization","name":"JieGG"}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body><div class="content"><div class="left-bar"><div class="left-bar__inner"><header class="header"><a class="logo" href="https://jie2gg.github.io/">博客 </a><a class="logo logo--atbottom" href="./">博客</a></header></div></div><main class="main"><article class="post"><div class="post__meta post__meta--attop"><div class="post__meta--attop__inner"><div class="post__maintag"><svg width="20" height="20" aria-hidden="true" focusable="false"><use xlink:href="https://jie2gg.github.io/assets/svg/svg-map.svg#tag"/></svg> Published in <a href="https://jie2gg.github.io/tags/ai/" class="metadata__maintag">AI</a></div></div></div><div class="main__inner"><div class="post__meta"><div class="post__author"><div><a href="https://jie2gg.github.io/authors/jiegg/" class="post__author__name">JieGG</a></div></div><div class="post__date"><time datetime="2024-04-25T23:49">24/04/25 23:49:51</time></div></div><header class="post__header"><h1 class="post__title">Qwen1.5大模型分布式部署</h1></header><div class="post__entry"><h1 id="概述">概述</h1><p>在当今人工智能的迅猛发展浪潮中，我们正见证着前所未有的模型规模与复杂度的飞跃。伴随着模型尺寸的指数级增长，传统的单机部署模式已渐渐显得捉襟见肘，面临着计算资源瓶颈、训练时间过长以及推理效率低下等诸多挑战。</p><p>分布式部署作为解锁大模型潜力的金钥匙。通过将模型分布在多个计算节点上并行处理，在推理阶段提供更高的吞吐量和更低的延迟，确保服务的可扩展性和可靠性。此外，分布式系统还能通过灵活的资源调配机制，实现成本效益的最大化。</p><p>本系列博客，将利用强大的 <a href="https://github.com/vllm-project/vllm">vLLM</a> 推理库与灵活的 <a href="https://github.com/ray-project">Ray</a> 集群，演示如何分布式部署 <a href="https://github.com/QwenLM">Qwen</a> 大模型。</p><hr><h1 id="软件要求">软件要求</h1><p>要顺利部署集群, 请按照以下系统要求进行配置</p><h5 id="已测试过的系统">已测试过的系统</h5><ul><li>基于 ProxmoxVE 8.0.4 的 Ubuntu 23.10 (LXC容器)</li></ul><h5 id="最低要求">最低要求</h5><ul><li>Python版本 &gt;= 3.11</li><li>CUDA版本 &gt;= 12.1</li><li>CUDAToolkit (与CUDA配套)</li><li>CUDNN (与CUDA配套)</li></ul><h1 id="部署方案">部署方案</h1><p><code>⚠️注意：集群部署必须保证每个计算节点上使用的模型、依赖、路径完全一致</code><br><code>⚠️注意：集群部署必须保证每个计算节点上使用的模型、依赖、路径完全一致</code><br><code>⚠️注意：集群部署必须保证每个计算节点上使用的模型、依赖、路径完全一致</code><br></p><h2 id="模型下载">模型下载</h2><p>首先将Qwen模型所需的模型下载至本地，通常开源的 LLM 模型可以从 <a href="https://huggingface.co/models">HuggingFace</a> 下载。</p><p>以本文章中使用的 LLM 模型 <a href="https://huggingface.co/Qwen/Qwen1.5-14B-Chat">Qwen/Qwen1.5-14B-Chat</a> 为例：</p><p>下载模型需要先安装 <a href="https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage">Git LFS</a></p><pre><code class="language-bash"># 安装 git-lfs
$ apt-get install git-lfs -y

# 下载所需的模型
$ git clone https://huggingface.co/Qwen/Qwen1.5-14B-Chat
</code></pre><h2 id="创建虚拟环境">创建虚拟环境</h2><p>Python3 虚拟环境是一种将 Python 项目及其依赖隔离的技术，它允许你在不同的项目中使用不同版本的库，而不会产生冲突。</p><pre><code class="language-bash"># 使用 apt-get 安装
$ apt-get install python3-venv

# 创建虚拟环境
$ python3 -m vevn venv_name

# 激活虚拟环境
$ source venv_name/bin/activate
</code></pre><h2 id="安装模型依赖">安装模型依赖</h2><p>安装 Qwen1.5 所需的依赖库，可以从HuggingFace安装 transformers 库。建议安装最新版本的 <code>transformers</code> 库，或至少安装 4.37.0 版本。</p><pre><code class="language-bash"># 通过 pip 安装
$ pip install transformers -U

# 通过源码安装
$ pip install git+https://github.com/huggingface/transformers
</code></pre><h2 id="安装-vllm">安装 vLLM</h2><p><a href="https://github.com/vllm-project/vllm">vLLM</a> 是一个开源库，用于加速大型语言模型（LLM）的推理服务。它易于使用，且具有最先进的服务吞吐量、高效的注意力键值内存管理（通过PagedAttention实现）、连续批处理输入请求、优化的CUDA内核等功能。要了解更多关于vLLM的信息，请参阅 <a href="https://arxiv.org/abs/2309.06180">论文</a> 和 <a href="https://vllm.readthedocs.io/">文档</a> 。</p><p>默认情况下，你可以通过 pip 来安装vLLM（vLLM&gt;=0.3.0），这里我们使用最新版本</p><pre><code class="language-bash"># 通过 pip 安装
$ pip install vllm
</code></pre><h2 id="安装-ray">安装 Ray</h2><p><a href="https://github.com/ray-project/ray">Ray</a> 是一个开源的分布式计算框架，它支持快速构建和运行高性能的分布式应用，包括大规模机器学习、数据处理和实时决策系统。要了解更多关于Ray的信息，请参阅 <a href="https://arxiv.org/abs/2203.05072">论文</a> 和 <a href="http://docs.ray.io/en/latest/index.html">文档</a></p><pre><code class="language-bash"># 通过 pip 安装
$ pip install &quot;ray[default]&quot;
</code></pre><h2 id="启动集群">启动集群</h2><p>在 Ray 分布式计算中，集群由几个关键组件构成，其中最重要的是 Head 节点和 Worker 节点。</p><ul><li>Head 节点</li></ul><p>Head 节点是 Ray 集群的控制中心，负责以下关键功能：</p><ol><li>集群管理：维护集群的状态，包括跟踪哪些 worker 节点已经连接、资源分配情况等。</li><li>任务调度：接收任务请求，根据各个 worker 节点的资源状况智能调度任务到合适的节点上执行。</li><li>对象存储：管理一个分布式对象存储系统，使得在不同节点上运行的任务可以高效地共享数据。</li><li>服务发现：提供一个接入点让新加入的 worker 节点或其他服务能够发现集群并加入。</li></ol><p>选中一台服务器作为 Head 节点，使用以下命令启动 Ray 集群服务</p><pre><code class="language-bash"># 启动 Ray 集群的 Head 节点
$ ray start --head --port=6379 --dashboard-host=0.0.0.0 --dashboard-port=8265
</code></pre><ul><li>Worker 节点</li></ul><p>Worker 节点是执行实际计算任务的单元，它们连接到 Head 节点并根据 Head 节点的指令执行以下操作：</p><ol><li>任务执行：接收 Head 节点分配的任务，执行计算任务并返回结果。</li><li>资源报告：向 Head 节点报告自身可用的资源（如：CPU、GPU、内存等），以便进行任务调度。</li><li>对象存储：从对象存储中获取任务所需的数据，并将执行结果写回，支持跨节点的数据共享。</li></ol><p>其余的服务器作为 Worker 节点，使用以下命令启动 Ray 集群服务</p><pre><code class="language-bash"># 启动 Ray 集群的 Worker 节点
# HEAD_IP 是实际的 Head 节点的 IP 地址
$ ray start --address=&#39;HEAD_IP:6379&#39;
</code></pre><h2 id="启动服务器">启动服务器</h2><p>借助 vLLM，构建一个与 OpenAI API 兼容的 API 服务十分简便，该服务可以作为实现OpenAI API协议的服务器进行部署。并且 vLLM 原生支持 Ray 集群，使用以下命令可以让大模型以集群的方式启动：</p><pre><code class="language-bash"># 多卡分布式启动 OpenAI API 服务器
$ python -m vllm.entrypoints.openai.api_server \
            --model Qwen/Qwen1.5-14B-Chat     # 模型位置
            --tensor-parallel-size 2                    # 使用的集群显卡数量
            --gpu-memory-utilization 0.9            # 使用90%的显存 
            --max-model-len 2048                     # 限制使用的模型长度，最大 32768
</code></pre></div><footer class="post__footer"><div class="post__last-updated">This article was updated on <time datetime="2024-04-26T10:36">24/04/26 10:36:45</time></div><div class="post__share"></div></footer></div></article><div class="post__section post__comments"><div class="main__inner"></div></div></main><div class="right-bar"><div class="right-bar__inner"><div class="sidebar"><section class="box authors"><h3 class="box__title">Authors</h3><ul class="authors__cotainer"><li class="authors__item"><div><a href="https://jie2gg.github.io/authors/jiegg/" class="authors__title">JieGG</a> <span class="authors__meta">Post: 1</span></div></li></ul></section><section class="box tags"><h3 class="box__title">Recommended Topics</h3><ul class="tags__list"><li class="tags__item"><a href="https://jie2gg.github.io/tags/ai/" class="btn btn--gray">AI <sup>(1)</sup></a></li><li class="tags__item"><a href="https://jie2gg.github.io/tags/ren-gong-zhi-neng/" class="btn btn--gray">运维 <sup>(1)</sup></a></li></ul></section><section class="box newsletter"><h3 class="box__title">Subscribe to Mono</h3><p class="newsletter__desc">Sign up to receive email updates and to hear what's going on with us!</p><form>...</form></section><div class="box copyright">Copyright © 2024 Jie2GG. All rights reserved.</div></div></div></div></div><script defer="defer" src="https://jie2gg.github.io/assets/js/scripts.min.js?v=12d8fcd46db8fdc7af6797ec26849875"></script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>