<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>大模型部署(二)——集群推理框架安装 - 博客</title><meta name="description" content="本文介绍 在部署大规模预训练模型时，选择合适的集群框架至关重要。本文将重点介绍 Xinference 和 Ray 这两个高效且易用的集群框架，并提供简要的安装指南。 在选择 Ray 集群和 Xinference 集群时，重要的是要考虑您的具体需求和应用场景。 Ray 是一个通用的分布式计算框架，支持多种类型的工作负载，包括机器学习、深度学习、强化学习等。 Xinference 是一个专为大模型推理优化的分布式推理框架。它特别设计用于提供高性能的推理服务，支持模型并行和数据并行，适用于需要高性能和低延迟响应的应用场景。 在选择集群时，如果您的项目需要一个通用的分布式计算框架来处理多种类型的任务，那么 Ray 将是一个理想的选择。而如果您专注于模型推理，并且需要一个高度优化的推理服务，那么 Xinference 将是更好的选择。根据您的具体需求和应用场景来决定使用哪一个集群，可以确保您的项目获得最佳性能和效率。 Ray 只是一个通用的框架，本身不能运行大语言模型。如果需要运行大模型，还需要使用 vLLM 借助 Ray 的能力进行分布式推理。 1. 环境准备： 2. 安装依赖： # 安装 python3 开发套件，虚拟环境 $ apt-get install python3-dev python3-venv libopenblas-dev 3. 安装Ray框架 # 创建 python3&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://jie2gg.github.io/da-mo-xing-bu-shuer-tui-li-kuang-jia-an-zhuang.html"><link rel="alternate" type="application/atom+xml" href="https://jie2gg.github.io/feed.xml"><link rel="alternate" type="application/json" href="https://jie2gg.github.io/feed.json"><link rel="stylesheet" href="https://jie2gg.github.io/assets/css/style.css?v=69aee13592669c1a8668a208ab37bc94"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://jie2gg.github.io/da-mo-xing-bu-shuer-tui-li-kuang-jia-an-zhuang.html"},"headline":"大模型部署(二)——集群推理框架安装","datePublished":"2024-08-16T11:33","dateModified":"2024-08-16T15:49","description":"本文介绍 在部署大规模预训练模型时，选择合适的集群框架至关重要。本文将重点介绍 Xinference 和 Ray 这两个高效且易用的集群框架，并提供简要的安装指南。 在选择 Ray 集群和 Xinference 集群时，重要的是要考虑您的具体需求和应用场景。 Ray 是一个通用的分布式计算框架，支持多种类型的工作负载，包括机器学习、深度学习、强化学习等。 Xinference 是一个专为大模型推理优化的分布式推理框架。它特别设计用于提供高性能的推理服务，支持模型并行和数据并行，适用于需要高性能和低延迟响应的应用场景。 在选择集群时，如果您的项目需要一个通用的分布式计算框架来处理多种类型的任务，那么 Ray 将是一个理想的选择。而如果您专注于模型推理，并且需要一个高度优化的推理服务，那么 Xinference 将是更好的选择。根据您的具体需求和应用场景来决定使用哪一个集群，可以确保您的项目获得最佳性能和效率。 Ray 只是一个通用的框架，本身不能运行大语言模型。如果需要运行大模型，还需要使用 vLLM 借助 Ray 的能力进行分布式推理。 1. 环境准备： 2. 安装依赖： # 安装 python3 开发套件，虚拟环境 $ apt-get install python3-dev python3-venv libopenblas-dev 3. 安装Ray框架 # 创建 python3&hellip;","author":{"@type":"Person","name":"JieGG","url":"https://jie2gg.github.io/authors/jiegg/"},"publisher":{"@type":"Organization","name":"JieGG"}}</script><style>.u-tag--ai{--badge-bg:#84888E;--badge-txt:#FFFFFF;}</style><noscript><style>img[loading]{opacity:1;}</style></noscript></head><body class="post-template"><header class="topbar" id="js-header"><div class="topbar__inner"><a class="logo" href="https://jie2gg.github.io/">博客</a></div></header><div class="main-content"><div class="infobar"><div class="infobar__update">Last Updated: <time datetime="2024-08-16T16:28">八月 16, 2024</time></div></div><main class="main post"><article class="content"><header class="u-header content__header"><h1>大模型部署(二)——集群推理框架安装</h1><div class="u-header__meta u-small"><div><a href="https://jie2gg.github.io/authors/jiegg/" title="JieGG">JieGG</a> <time datetime="2024-08-16T11:33">八月 16, 2024 </time><a href="https://jie2gg.github.io/tags/ai/" class="u-tag u-tag--ai">AI</a></div></div></header><div class="content__entry u-inner"><h3 id="本文介绍">本文介绍</h3><p>在部署大规模预训练模型时，选择合适的集群框架至关重要。本文将重点介绍 Xinference 和 Ray 这两个高效且易用的集群框架，并提供简要的安装指南。</p><h3 id="框架简介">框架简介</h3><p>在选择 Ray 集群和 Xinference 集群时，重要的是要考虑您的具体需求和应用场景。</p><ul><li><p>Ray 是一个通用的分布式计算框架，支持<strong>多种类型的工作负载</strong>，包括<strong>机器学习、深度学习、强化学习</strong>等。</p></li><li><p>Xinference 是一个专为大模型推理优化的分布式推理框架。它特别设计用于提供<strong>高性能的推理服务</strong>，支持<strong>模型并行和数据并行</strong>，适用于需要高性能和低延迟响应的应用场景。</p></li></ul><h3 id="框架选择">框架选择</h3><p>在选择集群时，如果您的项目需要一个通用的分布式计算框架来处理多种类型的任务，那么 Ray 将是一个理想的选择。而如果您专注于模型推理，并且需要一个高度优化的推理服务，那么 Xinference 将是更好的选择。根据您的具体需求和应用场景来决定使用哪一个集群，可以确保您的项目获得最佳性能和效率。</p><h3 id="ray集群部署">Ray集群部署</h3><p>Ray 只是一个通用的框架，本身不能运行大语言模型。如果需要运行大模型，还需要使用 vLLM 借助 Ray 的能力进行分布式推理。</p><p><strong>1. 环境准备：</strong></p><ul><li>确保您有一台或多台服务器或虚拟机，这些机器将被用作 Ray 集群的节点。</li><li>所有的机器都应该安装了相同版本的 Python（Ubuntu23.10自带Python3.11）</li></ul><p><strong>2. 安装依赖：</strong></p><pre><code class="language-bash"># 安装 python3 开发套件，虚拟环境
$ apt-get install python3-dev python3-venv libopenblas-dev
</code></pre><p><strong>3. 安装Ray框架</strong></p><pre><code class="language-bash"># 创建 python3 虚拟环境
$ python3 -m venv ray

# 激活虚拟环境
$ source ray/bin/activate

# 更新 pip3 版本并安装 ray 框架
$ pip3 install --upgrade pip
$ pip3 install &quot;ray[default]&quot;
</code></pre><p><strong>4. 安装vllm框架</strong></p><pre><code class="language-bash">$ pip3 install vllm
</code></pre><p><strong>5. 启动集群</strong></p><p>Ray 分布式集群由几个关键节点构成，其中最重要的是 Head 节点和 Worker 节点。</p><ul><li><strong>Head 节点</strong></li></ul><p>Head 节点是 Ray 集群的控制中心，负责以下关键功能：</p><ol><li>集群管理：维护集群的状态，包括跟踪哪些 worker 节点已经连接、资源分配情况等。</li><li>任务调度：接收任务请求，根据各个 worker 节点的资源状况智能调度任务到合适的节点上执行。</li><li>对象存储：管理一个分布式对象存储系统，使得在不同节点上运行的任务可以高效地共享数据。</li><li>服务发现：提供一个接入点让新加入的 worker 节点或其他服务能够发现集群并加入。</li></ol><p>选中一台服务器作为 Head 节点，使用以下命令启动 Ray 集群服务</p><pre><code class="language-bash"># 启动 Ray 集群的 Head 节点
$ ray start --head --port=6379 --dashboard-host=0.0.0.0 --dashboard-port=8265
</code></pre><ul><li><strong>Worker 节点</strong></li></ul><p>Worker 节点是执行实际计算任务的单元，它们连接到 Head 节点并根据 Head 节点的指令执行以下操作：</p><ol><li>任务执行：接收 Head 节点分配的任务，执行计算任务并返回结果。</li><li>资源报告：向 Head 节点报告自身可用的资源（如：CPU、GPU、内存等），以便进行任务调度。</li><li>对象存储：从对象存储中获取任务所需的数据，并将执行结果写回，支持跨节点的数据共享。</li></ol><p>其余的服务器作为 Worker 节点，使用以下命令启动 Ray 集群服务</p><pre><code class="language-bash"># 启动 Ray 集群的 Worker 节点
# HEAD_IP 是实际的 Head 节点的 IP 地址
$ ray start --address=&#39;HEAD_IP:6379&#39;
</code></pre><h3 id="xinference集群部署">Xinference集群部署</h3><p>Xinference 是专用于大模型的框架，内置了包含 vLLM 在内的多种推理引擎，支持部署包括 LLM，VLM，Embdding，Reranker 多种模型。</p><p><strong>1. 环境准备：</strong></p><ul><li>确保您有一台或多台服务器或虚拟机，这些机器将被用作 Xinference 集群的节点。</li><li>所有的机器都应该安装了相同版本的 Python（Ubuntu23.10自带Python3.11）</li></ul><p><strong>2. 安装依赖：</strong></p><pre><code class="language-bash"># 安装 python3 开发套件，虚拟环境
$ apt-get install python3-dev python3-venv libopenblas-dev
</code></pre><p><strong>3. 安装Xinference框架</strong></p><pre><code class="language-bash"># 创建 python3 虚拟环境
$ python3 -m venv xinference

# 激活虚拟环境
$ source xinference/bin/activate

# 更新 pip3 版本并安装 xinference 框架
$ pip3 install --upgrade pip
$ pip3 install &quot;xinference[all]&quot;
</code></pre><p><strong>4. 启动集群</strong></p><p>Xinference 集群由几个关键节点构成，其中最重要的是 Supervisor 节点和 Worker 节点。</p><ul><li><strong>Supervisor 节点</strong></li></ul><p>Supervisor 节点是 Xinference 集群的控制中心，负责以下关键功能：</p><ol><li>任务调度：Supervisor 节点负责调度推理任务到集群中的工作节点上执行。</li><li>资源管理：它管理集群中可用的计算资源（如 CPU、GPU、内存等），并根据任务需求动态分配资源。</li><li>状态监控：Supervisor 节点监控集群中所有节点的状态，确保任务正常运行，并在出现问题时采取相应的行动。</li><li>故障恢复：当工作节点出现故障时，Supervisor 节点能够检测到这一情况，并重新调度任务到其他可用节点上。</li><li>配置管理：Supervisor 节点还可能负责配置管理，包括设置每个节点的运行参数和环境变量。</li><li>性能优化：根据集群的负载情况，Supervisor 节点可以进行动态调整以优化整体性能。</li><li>API 接口：提供 API 接口供外部系统查询集群状态、提交任务、获取结果等。</li></ol><p><code>⚠️注意：Supervisor 节点只负责调度及提供API接口</code><br>选中一台服务器作为 Supervisor 节点，使用以下命令启动 Xinference 集群服务</p><pre><code class="language-bash"># 启动 Xinference 集群的 Supervisor 节点，SUPERVISOR_HOST 替换为当前节点内网IP
$ xinference-supervisor -H &quot;SUPERVISOR_HOST&quot;
</code></pre><ul><li><strong>Worker 节点</strong></li></ul><p>Worker 节点是执行实际计算任务的单元，它们连接到 Supervisor 节点并根据 Supervisor 节点的指令执行以下操作：</p><ol><li>模型部署：接收 Supervisor 节点分配的任务，从网络下载模型并部署。</li><li>资源报告：向 Supervisor 节点报告自身可用的资源（如：CPU、GPU、内存等），以便进行任务调度。</li></ol><p>其余的服务器作为 Worker 节点，使用以下命令启动 Xinference 集群服务</p><pre><code class="language-bash"># 启动 Xinference 集群的 Worker 节点，SUPERVISOR_HOST替换为 Supervisor 节点内网IP，WORKER_HOST 替换为当前接待内网IP
$ xinference-worker -e &quot;http://SUPERVISOR_HOST:9997&quot; -H &quot;WORKER_HOST&quot;
</code></pre></div><aside class="content__aside"><div class="content__last-updated u-small">This article was updated on 八月 16, 2024</div><div class="content__share"></div></aside><footer class="content__footer"><div class="content__bio box"><div><h4 class="h6"><a href="https://jie2gg.github.io/authors/jiegg/" title="JieGG">JieGG</a></h4></div></div><nav class="content__nav box"><div class="content__nav__prev"><a href="https://jie2gg.github.io/ji-qun-bu-shu-da-mo-xingyi-fu-wu-qi-huan-jing-an-zhuang.html" class="content__nav__link" rel="prev"><div class="u-small">Previous Post<h5>大模型部署(一)——服务器环境安装</h5></div></a></div><div class="content__nav__next"><a href="https://jie2gg.github.io/da-mo-xing-bu-shusan-mo-xing-qi-dong.html" class="content__nav__link" rel="next"><div class="u-small">Next Post<h5>大模型部署(三)——模型启动</h5></div></a></div></nav></footer></article></main><div class="sidebar"><section class="box authors"><h3 class="box__title">Authors</h3><ul class="authors__cotainer"><li class="authors__item"><div><a href="https://jie2gg.github.io/authors/jiegg/" class="authors__title">JieGG</a> <span class="u-small">Post: 3</span></div></li></ul></section><section class="newsletter box box--gray"><h3 class="box__title">Newsletter</h3><p class="newsletter__description">Sign up to receive email updates and to hear what's going on with us!</p><form>...</form></section><section class="box"><h3 class="box__title">Tags</h3><ul class="tags"><li><a href="https://jie2gg.github.io/tags/ai/">AI</a> <span class="u-small">(3)</span></li></ul></section></div><footer class="footer"><a class="footer__logo" href="https://jie2gg.github.io/">博客</a><nav><ul class="footer__nav"></ul></nav><div class="footer__copyright">Powered by Publii</div></footer></div><script>window.publiiThemeMenuConfig = {    
        mobileMenuMode: 'sidebar',
        animationSpeed: 300,
        submenuWidth: 'auto',
        doubleClickTime: 500,
        mobileMenuExpandableSubmenus: true, 
        relatedContainerForOverlayMenuSelector: '.navbar',
   };</script><script defer="defer" src="https://jie2gg.github.io/assets/js/scripts.min.js?v=18a65c58aef57c0287e0f0609f3389a9"></script><script>var images = document.querySelectorAll('img[loading]');

      for (var i = 0; i < images.length; i++) {
         if (images[i].complete) {
               images[i].classList.add('is-loaded');
         } else {
               images[i].addEventListener('load', function () {
                  this.classList.add('is-loaded');
               }, false);
         }
      }</script></body></html>