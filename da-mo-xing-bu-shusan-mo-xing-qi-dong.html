<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>大模型部署(三)——模型启动 - 博客</title><meta name="description" content="本文介绍 由于大模型的复杂性和计算需求，启动和运行这些模型往往需要大量的计算资源和高效的分布式架构。Xinference 和 vLLM 是两个旨在简化大模型部署和提高推理性能的工具。 1. 模型下载 首先将模型所需的模型下载至本地，通常开源的 LLM 模型可以从 HuggingFace 或 ModelScope 下载。 以本文章中使用的 LLM 模型 Qwen2-72B-Instruct 为例： 下载模型需要先安装 Git LFS # 安装 git-lfs $ apt-get install git-lfs -y # 下载所需的模型 $ git clone https://www.modelscope.cn/models/qwen/qwen2-72b-instruct 2. 模型启动 借助 vLLM，构建一个与 OpenAI API 兼容的 API 服务十分简便，该服务可以作为实现OpenAI&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://jie2gg.github.io/da-mo-xing-bu-shusan-mo-xing-qi-dong.html"><link rel="alternate" type="application/atom+xml" href="https://jie2gg.github.io/feed.xml"><link rel="alternate" type="application/json" href="https://jie2gg.github.io/feed.json"><link rel="stylesheet" href="https://jie2gg.github.io/assets/css/style.css?v=69aee13592669c1a8668a208ab37bc94"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://jie2gg.github.io/da-mo-xing-bu-shusan-mo-xing-qi-dong.html"},"headline":"大模型部署(三)——模型启动","datePublished":"2024-08-16T15:49","dateModified":"2024-08-16T16:28","description":"本文介绍 由于大模型的复杂性和计算需求，启动和运行这些模型往往需要大量的计算资源和高效的分布式架构。Xinference 和 vLLM 是两个旨在简化大模型部署和提高推理性能的工具。 1. 模型下载 首先将模型所需的模型下载至本地，通常开源的 LLM 模型可以从 HuggingFace 或 ModelScope 下载。 以本文章中使用的 LLM 模型 Qwen2-72B-Instruct 为例： 下载模型需要先安装 Git LFS # 安装 git-lfs $ apt-get install git-lfs -y # 下载所需的模型 $ git clone https://www.modelscope.cn/models/qwen/qwen2-72b-instruct 2. 模型启动 借助 vLLM，构建一个与 OpenAI API 兼容的 API 服务十分简便，该服务可以作为实现OpenAI&hellip;","author":{"@type":"Person","name":"JieGG","url":"https://jie2gg.github.io/authors/jiegg/"},"publisher":{"@type":"Organization","name":"JieGG"}}</script><style>.u-tag--ai{--badge-bg:#84888E;--badge-txt:#FFFFFF;}</style><noscript><style>img[loading]{opacity:1;}</style></noscript></head><body class="post-template"><header class="topbar" id="js-header"><div class="topbar__inner"><a class="logo" href="https://jie2gg.github.io/">博客</a></div></header><div class="main-content"><div class="infobar"><div class="infobar__update">Last Updated: <time datetime="2024-08-16T16:28">八月 16, 2024</time></div></div><main class="main post"><article class="content"><header class="u-header content__header"><h1>大模型部署(三)——模型启动</h1><div class="u-header__meta u-small"><div><a href="https://jie2gg.github.io/authors/jiegg/" title="JieGG">JieGG</a> <time datetime="2024-08-16T15:49">八月 16, 2024 </time><a href="https://jie2gg.github.io/tags/ai/" class="u-tag u-tag--ai">AI</a></div></div></header><div class="content__entry u-inner"><h3 id="本文介绍">本文介绍</h3><p>由于大模型的复杂性和计算需求，启动和运行这些模型往往需要大量的计算资源和高效的分布式架构。Xinference 和 vLLM 是两个旨在简化大模型部署和提高推理性能的工具。</p><h3 id="vllm集群部署">vLLM集群部署</h3><p><strong>1. 模型下载</strong></p><p>首先将模型所需的模型下载至本地，通常开源的 LLM 模型可以从 <a href="https://huggingface.co/models">HuggingFace</a> 或 <a href="https://www.modelscope.cn/">ModelScope</a> 下载。</p><p>以本文章中使用的 LLM 模型 <a href="https://www.modelscope.cn/models/qwen/qwen2-72b-instruct">Qwen2-72B-Instruct</a> 为例：</p><p>下载模型需要先安装 <a href="https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage">Git LFS</a></p><pre><code class="language-bash"># 安装 git-lfs
$ apt-get install git-lfs -y

# 下载所需的模型
$ git clone https://www.modelscope.cn/models/qwen/qwen2-72b-instruct
</code></pre><p><strong>2. 模型启动</strong></p><p>借助 vLLM，构建一个与 OpenAI API 兼容的 API 服务十分简便，该服务可以作为实现OpenAI API协议的服务器进行部署。并且 vLLM 原生支持 Ray 集群，使用以下命令可以让大模型以集群的方式启动：</p><pre><code class="language-bash"># 多卡分布式启动 OpenAI API 服务器
$ python -m vllm.entrypoints.openai.api_server \
      --served-model-name Qwen2-72B-Instruct \
            --model path/to/weight \
            --pipeline-parallel-size 2 \
            --tensor-parallel-size 1 \
            --gpu-memory-utilization 0.98 \
            --max-model-len 32768 \
</code></pre><p>参数说明：</p><ul><li>served-model-name 模型ID，请求时的模型名称</li><li>model 模型路径，模型在集群上的位置</li><li>pipeline-parallel-size 管线数量，例：2个节点设置为2</li><li>tensor-parallel-size 节点GPU卡数量，每个节点的GPU数量</li><li>gpu-memory-utilization GPU显存占用量，最大值 1。0.98 表示占用98%</li><li>max-model-len 最大模型大小，限制模型的尺寸以降低显存占用</li></ul><h3 id="xinference集群部署">Xinference集群部署</h3><p><strong>1. 访问平台</strong> 访问 Supervisor 节点，在浏览器中访问 “<a href="http://SUPERVISOR_HOST:9997">http://SUPERVISOR_HOST:9997</a>“ （SUPERVISOR_HOST 替换为 Supervisor 的IP地址） 界面如下：</p><figure class="post__image"><img loading="lazy" src="https://jie2gg.github.io/media/posts/6/xinference-ui-2.png" alt="xinference-ui" width="2914" height="1726" sizes="(max-width: 48em) 100vw, 768px" srcset="https://jie2gg.github.io/media/posts/6/responsive/xinference-ui-2-xs.png 300w, https://jie2gg.github.io/media/posts/6/responsive/xinference-ui-2-sm.png 480w, https://jie2gg.github.io/media/posts/6/responsive/xinference-ui-2-md.png 768w, https://jie2gg.github.io/media/posts/6/responsive/xinference-ui-2-lg.png 1024w, https://jie2gg.github.io/media/posts/6/responsive/xinference-ui-2-xl.png 1360w, https://jie2gg.github.io/media/posts/6/responsive/xinference-ui-2-2xl.png 1600w"></figure><p></p><p><strong>2. 启动模型</strong> 点击要启动的模型，选择 vLLM 引擎，设置模型参数（可参考 vLLM 的启动参数），点击启动图标后 xinference 会自动下载并部署模型。</p><figure class="post__image"><img loading="lazy" src="https://jie2gg.github.io/media/posts/6/xinference-launch.png" alt="xinference-launch" width="2914" height="1726" sizes="(max-width: 48em) 100vw, 768px" srcset="https://jie2gg.github.io/media/posts/6/responsive/xinference-launch-xs.png 300w, https://jie2gg.github.io/media/posts/6/responsive/xinference-launch-sm.png 480w, https://jie2gg.github.io/media/posts/6/responsive/xinference-launch-md.png 768w, https://jie2gg.github.io/media/posts/6/responsive/xinference-launch-lg.png 1024w, https://jie2gg.github.io/media/posts/6/responsive/xinference-launch-xl.png 1360w, https://jie2gg.github.io/media/posts/6/responsive/xinference-launch-2xl.png 1600w"></figure><p></p></div><aside class="content__aside"><div class="content__last-updated u-small">This article was updated on 八月 16, 2024</div><div class="content__share"></div></aside><footer class="content__footer"><div class="content__bio box"><div><h4 class="h6"><a href="https://jie2gg.github.io/authors/jiegg/" title="JieGG">JieGG</a></h4></div></div><nav class="content__nav box"><div class="content__nav__prev"><a href="https://jie2gg.github.io/da-mo-xing-bu-shuer-tui-li-kuang-jia-an-zhuang.html" class="content__nav__link" rel="prev"><div class="u-small">Previous Post<h5>大模型部署(二)——集群推理框架安装</h5></div></a></div></nav></footer></article></main><div class="sidebar"><section class="box authors"><h3 class="box__title">Authors</h3><ul class="authors__cotainer"><li class="authors__item"><div><a href="https://jie2gg.github.io/authors/jiegg/" class="authors__title">JieGG</a> <span class="u-small">Post: 3</span></div></li></ul></section><section class="newsletter box box--gray"><h3 class="box__title">Newsletter</h3><p class="newsletter__description">Sign up to receive email updates and to hear what's going on with us!</p><form>...</form></section><section class="box"><h3 class="box__title">Tags</h3><ul class="tags"><li><a href="https://jie2gg.github.io/tags/ai/">AI</a> <span class="u-small">(3)</span></li></ul></section></div><footer class="footer"><a class="footer__logo" href="https://jie2gg.github.io/">博客</a><nav><ul class="footer__nav"></ul></nav><div class="footer__copyright">Powered by Publii</div></footer></div><script>window.publiiThemeMenuConfig = {    
        mobileMenuMode: 'sidebar',
        animationSpeed: 300,
        submenuWidth: 'auto',
        doubleClickTime: 500,
        mobileMenuExpandableSubmenus: true, 
        relatedContainerForOverlayMenuSelector: '.navbar',
   };</script><script defer="defer" src="https://jie2gg.github.io/assets/js/scripts.min.js?v=18a65c58aef57c0287e0f0609f3389a9"></script><script>var images = document.querySelectorAll('img[loading]');

      for (var i = 0; i < images.length; i++) {
         if (images[i].complete) {
               images[i].classList.add('is-loaded');
         } else {
               images[i].addEventListener('load', function () {
                  this.classList.add('is-loaded');
               }, false);
         }
      }</script></body></html>